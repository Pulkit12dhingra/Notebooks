{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    def type_of_cols(sf,data):\n",
    "        # if the availabe no of unique values of a column is \n",
    "        #greater than 5 then it can be classified as continues\n",
    "        #else as categorial\n",
    "        col_type_and_name=[]\n",
    "        col_type=[]\n",
    "        no_of_unique=5\n",
    "        for col in data.columns[:-1]:\n",
    "            no_of_col_unique= data[col].nunique()\n",
    "            if data[col].dtypes==object or no_of_col_unique<no_of_unique:\n",
    "                col_type_and_name.append((col,\"categorical\"))\n",
    "                col_type.append(\"categorical\")\n",
    "            else:\n",
    "                col_type_and_name.append((col,\"continuous\"))\n",
    "                col_type.append(\"continuous\")\n",
    "        return col_type\n",
    "    def get_splits(sf,data):\n",
    "        splits = {} # dict to store the possible splits of our data based on either categorial or continues data\n",
    "        feature_type=sf.type_of_cols(data) # get the type of data of each column of feature set\n",
    "        for column_index in range(data.shape[1]-1):          # excluding the last column which is the label\n",
    "            values = data.iloc[:, column_index] # accessing the all the column value based on index nor the column name\n",
    "            unique_values = np.unique(values) #fetch the unique values of the respective column\n",
    "            type_of_feature = feature_type[column_index] # get data type of column i.e categorial or continues\n",
    "            if type_of_feature == \"continuous\":\n",
    "                splits[column_index] = []\n",
    "                for index in range(len(unique_values)):\n",
    "                    if index != 0:\n",
    "                        current_value = unique_values[index]\n",
    "                        previous_value = unique_values[index - 1]\n",
    "                        split = (current_value + previous_value) / 2\n",
    "                        splits[column_index].append(split)\n",
    "            # feature is categorical(there need to be at least 2 unique values, otherwise in the \n",
    "            #split_data function data_below would contain all data points and data_above would be empty)\n",
    "            elif len(unique_values) > 1:\n",
    "                splits[column_index] = unique_values\n",
    "        return splits\n",
    "    def split_data(sf,data, column, value):\n",
    "        #based on to column's data type we will destribute the data into two portion's \n",
    "        feature_type=sf.type_of_cols(data)\n",
    "        split_values = data.iloc[:,column]\n",
    "        type_of_feature = feature_type[column]\n",
    "        if type_of_feature == \"continuous\": # in case of continues data we will use greater or lesser than operator\n",
    "            left = data[split_values <= value]\n",
    "            right = data[split_values >  value]\n",
    "        else:# in case of categorial data will use logical equal or not equal operator\n",
    "            left = data[split_values == value]\n",
    "            right = data[split_values != value]\n",
    "        return left,right\n",
    "\n",
    "    def entropy(sf,data):#we are calculating the entropy of class \n",
    "        prob=list(dict(data.iloc[:, -1].value_counts(normalize=True)).values())\n",
    "        entropy = sum(prob* -np.log2(prob))    \n",
    "        return entropy\n",
    "\n",
    "    def entropy_data(sf,left,right):    \n",
    "        n = len(left) + len(right)\n",
    "        p_left = len(left) / n\n",
    "        p_right = len(right) / n\n",
    "        entropy_ =  (p_left * sf.entropy(left)+ p_right *sf.entropy(right))\n",
    "        return entropy_\n",
    "    \n",
    "    def best_split(sf,data,splits):\n",
    "        entropy = 9999 #we assume a dummy entropy to compare with obtained entropy for the first time \n",
    "        for col in splits: # iterating over the splits obtained by the get_split method for each columns\n",
    "            for val in splits[col]: # iterating over the splits of a indivisual columns\n",
    "                left, right = sf.split_data(data, column=col, value=val)# spliting the data according to obtained split(val) of a column\n",
    "                current_entropy = sf.entropy_data(left,right) #calculating the entropy of a column\n",
    "                if current_entropy <= entropy: # if obtained entropy is lesser than assumed entropy \n",
    "                    entropy = current_entropy #then assume the obtained entropy as best entropy\n",
    "                    best_column = col # the current column can be termend as best column and the split too\n",
    "                    best_split = val\n",
    "        return best_column, best_split\n",
    "    \n",
    "    def tree_builder(sf,df, counter=0, min_samples=2, max_depth=5):\n",
    "        from collections import Counter\n",
    "        # data preparations\n",
    "        column=df.columns #store the column name\n",
    "        feature_type=sf.type_of_cols(df) # store the column value type i.e categorial/continues \n",
    "        data = df\n",
    "        # base cases\n",
    "        if (df.iloc[:,-1].nunique()==1) or (len(data) < min_samples) or (counter == max_depth):\n",
    "            classes= Counter(data.iloc[:,-1]).most_common(1)[0][0]\n",
    "            return classes\n",
    "        # recursive part\n",
    "        else:    \n",
    "            counter += 1\n",
    "            splits = sf.get_splits(data)#calculating the splits of each columns\n",
    "            split_column, split_value = sf.best_split(data,splits) #getting the best column and split value\n",
    "            left, right = sf.split_data(data, split_column, split_value) # based on the above split and column divide the data \n",
    "            # determine question\n",
    "            feature_name = column[split_column] #pick the column name\n",
    "            type_of_feature = feature_type[split_column]\n",
    "            if type_of_feature == \"continuous\":\n",
    "                question = \"{} <= {}\".format(feature_name, split_value) # assign the column name ,type of operator to be used and the split value            \n",
    "            # feature is categorical\n",
    "            else:\n",
    "                question = \"{} = {}\".format(feature_name, split_value)\n",
    "            # instantiate sub-tree\n",
    "            mytree = {question: []}\n",
    "            # find answers (recursion)\n",
    "            ans_yes = sf.tree_builder(left, counter, min_samples, max_depth) # left leave is for yes where tree traversal stops\n",
    "            ans_no = sf.tree_builder(right, counter, min_samples, max_depth) # right leave needs few more nodes\n",
    "            # If the answers are the same, then there is no point in asking the question.\n",
    "            # This could happen when the data is classified even though it is not pure\n",
    "            # yet (min_samples or max_depth base case).\n",
    "            if ans_yes == ans_no:\n",
    "                mytree = ans_yes\n",
    "            else:\n",
    "                mytree[question].append(ans_yes)\n",
    "                mytree[question].append(ans_no)\n",
    "        \n",
    "            return mytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d <= 2.5': [{'b <= 5.5': [{'g <= 4.5': [2, {'e = 3': [4, 2]}]},\n",
       "    {'g <= 1.5': [2, 4]}]},\n",
       "  {'c <= 1.5': [{'i = 10': [4, 2]},\n",
       "    {'b <= 6.5': [{'g <= 6.0': [{'i <= 2.5': [2, 4]}, {'g <= 11.0': [4, 2]}]},\n",
       "      4]}]}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('bcancer.csv')\n",
    "data.drop([\"Unnamed: 0\",'a'],axis=1,inplace=True)\n",
    "train=data[:int(len(data)*0.8)]\n",
    "test=data[int(len(data)*0.8):]\n",
    "algo=Tree()\n",
    "#algo.best_split(train,algo.get_splits(train)),algo.entropy_data(*algo.split_data(train,2,2.5))\n",
    "mytree=algo.tree_builder(train)\n",
    "mytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c <= 1.5': [{'i = 10': [4, 2]},\n",
       "  {'b <= 6.5': [{'g <= 6.0': [{'i <= 2.5': [2, 4]}, {'g <= 11.0': [4, 2]}]},\n",
       "    4]}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytree['d <= 2.5'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(Tree):\n",
    "    def fit(sf,train):\n",
    "        sf.tree=sf.tree_builder(train)\n",
    "        return sf.tree\n",
    "    def predict(sf,dx,tree):\n",
    "        root_node = list(tree.keys())[0] # fetch the root node's value i.e our dict keys which consits of column name,operator and split value \n",
    "        column,operator,split=root_node.split(\" \") # we have used the space as a seprator b/w the three data\n",
    "        if operator == \"<=\": # if the operator is lesser than or equal it means th column type is continues\n",
    "            \n",
    "            if dx[column] <= float(split):\n",
    "                result = tree[root_node][0]\n",
    "            else:\n",
    "                result = tree[root_node][1]\n",
    "        # if column  is categorical then we can use logical equal operator\n",
    "        else:\n",
    "            if str(dx[column]) == split:\n",
    "                result = tree[root_node][0]\n",
    "            else:\n",
    "                result = tree[root_node][1]\n",
    "        if type(result)!=dict: # if the result is dict then we have more nodes to be traversed\n",
    "            return result\n",
    "        # else recursively travers the entire tree for accurate results\n",
    "        else:\n",
    "            return sf.predict(dx,result)\n",
    "    def score(sf,x,y):\n",
    "        c=0 # we will pass a indivisual x into predict and compare the given y with the predicted y if both are equal then c will be incremented by 1 \n",
    "        for i in range(len(x)):\n",
    "            if sf.predict(x.iloc[i],sf.tree)==y.iloc[i]:\n",
    "                c+=1\n",
    "        return c/len(x) # finally resultant is correct prediction by the total len of x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9785714285714285"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myalgo=DecisionTree()\n",
    "myalgo.fit(train)\n",
    "myalgo.score(test.drop('class',axis=1),test['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
